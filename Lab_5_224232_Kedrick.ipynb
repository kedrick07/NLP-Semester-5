{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kedrick07/NLP-Semester-5/blob/main/Lab_5_224232_Kedrick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 5: N-gram Language Processing**\n",
        "**Course:** SKM3206 â€“ Natural Language Processing  \n",
        "**Lecturer:** Assoc. Prof. Dr. Azreen Azman / Dr. Nurul Amelina Nasharuddin  \n",
        "**Due date:** 23 November 2025 (Sunday)\n",
        "\n",
        "> âš ï¸ **Instructions:**  \n",
        "> 1. Go to *File â†’ Save a copy in Drive* before editing.  \n",
        "> 2. Rename your notebook as `Lab_LabNo_StudentID_Name.ipynb`.\n",
        "> 3. Read the examples carefully before attempting the exercises.\n",
        "> 4. Complete all the tasks in the code cells provided.  \n",
        "> 5. Submit your Colab link (e.g. http://colab.research.google.com/drive/....) as a submission in the PutraBlast.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vCExb-DsYsOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ðŸ’» Activity**"
      ],
      "metadata": {
        "id": "nM94F-l6hPvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review and implement the Python scripts in this [link](https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853) to build an n-gram language model and test it on the next word prediction. Read the 'add-1 smoothing' technique as well. Implement all codes in the code cell below."
      ],
      "metadata": {
        "id": "7E9J0qjV_3Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# ========================================\n",
        "# STEP 1: LOAD AND PREPROCESS DATA\n",
        "# ========================================\n",
        "\n",
        "# Sample corpus from the Medium article\n",
        "sample_corpus = \"\"\"\n",
        "Alice was beginning to get very tired of sitting by her sister on the bank,\n",
        "and of having nothing to do: once or twice she had peeped into the book her\n",
        "sister was reading, but it had no pictures or conversations in it, and what\n",
        "is the use of a book, thought Alice without pictures or conversations. So she\n",
        "was considering in her own mind as well as she could, for the hot day made her\n",
        "feel very sleepy and stupid, whether the pleasure of making a daisy chain would\n",
        "be worth the trouble of getting up and picking the daisies, when suddenly a\n",
        "White Rabbit with pink eyes ran close by her. There was nothing so very remarkable\n",
        "in that; nor did Alice think it so very much out of the way to hear the Rabbit\n",
        "say to itself, Oh dear! Oh dear! I shall be late! when she thought it over afterwards,\n",
        "it occurred to her that she ought to have wondered at this, but at the time it all\n",
        "seemed quite natural; but when the Rabbit actually took a watch out of its waistcoat\n",
        "pocket, and looked at it, and then hurried on, Alice started to her feet, for it\n",
        "flashed across her mind that she had never before seen a rabbit with either a\n",
        "waistcoat pocket, or a watch to take out of it, and burning with curiosity, she\n",
        "ran across the field after it, and fortunately was just in time to see it pop down\n",
        "a large rabbit hole under the hedge.\n",
        "\"\"\"\n",
        "\n",
        "# FIXED: Preprocess properly - preserve spaces\n",
        "file = sample_corpus.replace(\"\\n\", \" \")\n",
        "file = re.sub(r'\\s+', ' ', file).strip()  # Normalize whitespace\n",
        "\n",
        "# Tokenize BEFORE removing punctuation\n",
        "sents = nltk.sent_tokenize(file)\n",
        "words = nltk.word_tokenize(file)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CORPUS STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"The number of sentences is {len(sents)}\")\n",
        "print(f\"The number of tokens is {len(words)}\")\n",
        "\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(f\"The average number of tokens per sentence is {average_tokens}\")\n",
        "\n",
        "unique_tokens = set([w.lower() for w in words if w.isalnum()])\n",
        "print(f\"The number of unique tokens are {len(unique_tokens)}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ========================================\n",
        "# STEP 2: BUILD N-GRAM MODELS\n",
        "# ========================================\n",
        "\n",
        "print(\"\\nBUILDING N-GRAM MODELS...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "unigram = []\n",
        "bigram = []\n",
        "trigram = []\n",
        "fourgram = []\n",
        "tokenized_text = []\n",
        "\n",
        "for sentence in sents:\n",
        "    sentence = sentence.lower()\n",
        "    sequence = word_tokenize(sentence)\n",
        "    # Remove punctuation but keep words\n",
        "    sequence = [word for word in sequence if word.isalnum()]\n",
        "\n",
        "    unigram.extend(sequence)\n",
        "    tokenized_text.append(sequence)\n",
        "    bigram.extend(list(ngrams(sequence, 2)))\n",
        "    trigram.extend(list(ngrams(sequence, 3)))\n",
        "    fourgram.extend(list(ngrams(sequence, 4)))\n",
        "\n",
        "# Function to remove ngrams containing only stopwords\n",
        "def removal(x):\n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in stop_words:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if count == 1:\n",
        "            y.append(pair)\n",
        "    return y\n",
        "\n",
        "bigram_filtered = removal(bigram)\n",
        "trigram_filtered = removal(trigram)\n",
        "fourgram_filtered = removal(fourgram)\n",
        "\n",
        "freq_bi = nltk.FreqDist(bigram_filtered)\n",
        "freq_tri = nltk.FreqDist(trigram_filtered)\n",
        "freq_four = nltk.FreqDist(fourgram_filtered)\n",
        "\n",
        "print(\"\\nMost common n-grams without stopword removal and without add-1 smoothing:\")\n",
        "print(\"=\"*60)\n",
        "print(\"Most common bigrams:\", freq_bi.most_common(5))\n",
        "print(\"\\nMost common trigrams:\", freq_tri.most_common(5))\n",
        "print(\"\\nMost common fourgrams:\", freq_four.most_common(5))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# N-grams with stopword removal\n",
        "print(\"\\n\\nMost common n-grams with stopword removal and without add-1 smoothing:\")\n",
        "print(\"=\"*60)\n",
        "unigram_sw_removed = [p for p in unigram if p not in stop_words]\n",
        "fdist = nltk.FreqDist(unigram_sw_removed)\n",
        "print(\"Most common unigrams:\", fdist.most_common(10))\n",
        "\n",
        "bigram_sw_removed = list(ngrams(unigram_sw_removed, 2))\n",
        "fdist = nltk.FreqDist(bigram_sw_removed)\n",
        "print(\"\\nMost common bigrams:\", fdist.most_common(10))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ========================================\n",
        "# STEP 3: ADD-1 SMOOTHING\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\\nAPPLYING ADD-1 SMOOTHING...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create n-grams for all orders\n",
        "ngrams_all = {1: [], 2: [], 3: [], 4: []}\n",
        "for i in range(4):\n",
        "    for each in tokenized_text:\n",
        "        for j in ngrams(each, i+1):\n",
        "            ngrams_all[i+1].append(j)\n",
        "\n",
        "# Get vocabulary for each n-gram order\n",
        "ngrams_voc = {1: set([]), 2: set([]), 3: set([]), 4: set([])}\n",
        "for i in range(4):\n",
        "    for gram in ngrams_all[i+1]:\n",
        "        if gram not in ngrams_voc[i+1]:\n",
        "            ngrams_voc[i+1].add(gram)\n",
        "\n",
        "# Count total n-grams and vocabulary size\n",
        "total_ngrams = {1: 0, 2: 0, 3: 0, 4: 0}\n",
        "total_voc = {1: 0, 2: 0, 3: 0, 4: 0}\n",
        "for i in range(4):\n",
        "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
        "    total_voc[i+1] = len(ngrams_voc[i+1])\n",
        "\n",
        "# Calculate smoothed probabilities\n",
        "ngrams_prob = {1: [], 2: [], 3: [], 4: []}\n",
        "for i in range(4):\n",
        "    for ngram in ngrams_voc[i+1]:\n",
        "        tlist = [ngram]\n",
        "        tlist.append(ngrams_all[i+1].count(ngram))\n",
        "        ngrams_prob[i+1].append(tlist)\n",
        "\n",
        "# Apply Add-1 smoothing\n",
        "for i in range(4):\n",
        "    for ngram in ngrams_prob[i+1]:\n",
        "        ngram[-1] = (ngram[-1] + 1) / (total_ngrams[i+1] + total_voc[i+1])\n",
        "\n",
        "# Sort and display\n",
        "print(\"\\nMost common n-grams without stopword removal and with add-1 smoothing:\")\n",
        "print(\"=\"*60)\n",
        "for i in range(4):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Most common unigrams:\", str(ngrams_prob[1][:10]))\n",
        "print(\"\\nMost common bigrams:\", str(ngrams_prob[2][:10]))\n",
        "print(\"\\nMost common trigrams:\", str(ngrams_prob[3][:10]))\n",
        "print(\"\\nMost common fourgrams:\", str(ngrams_prob[4][:10]))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ========================================\n",
        "# STEP 4: NEXT WORD PREDICTION\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\\nNEXT WORD PREDICTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test strings\n",
        "str1 = 'alice was very'\n",
        "str2 = 'she ran across the'\n",
        "\n",
        "# Tokenize and clean test strings\n",
        "token_1 = [w.lower() for w in word_tokenize(str1) if w.isalnum()]\n",
        "token_2 = [w.lower() for w in word_tokenize(str2) if w.isalnum()]\n",
        "\n",
        "# Get n-grams from test strings\n",
        "ngram_1 = {1: [], 2: [], 3: []}\n",
        "ngram_2 = {1: [], 2: [], 3: []}\n",
        "\n",
        "for i in range(3):\n",
        "    ngram_1[i+1] = tuple(token_1[-(i+1):])\n",
        "    ngram_2[i+1] = tuple(token_2[-(i+1):])\n",
        "\n",
        "print(f\"String 1 n-grams: {ngram_1}\")\n",
        "print(f\"String 2 n-grams: {ngram_2}\\n\")\n",
        "\n",
        "# Predict next words for string 1\n",
        "pred_1 = {1: [], 2: [], 3: []}\n",
        "for i in range(3):\n",
        "    count = 0\n",
        "    for each in ngrams_prob[i+2]:\n",
        "        if each[0][:-1] == ngram_1[i+1]:\n",
        "            count += 1\n",
        "            pred_1[i+1].append(each[0][-1])\n",
        "            if count == 5:\n",
        "                break\n",
        "    if count < 5:\n",
        "        while count != 5:\n",
        "            pred_1[i+1].append(\"NOT FOUND\")\n",
        "            count += 1\n",
        "\n",
        "# Predict next words for string 2\n",
        "pred_2 = {1: [], 2: [], 3: []}\n",
        "for i in range(3):\n",
        "    count = 0\n",
        "    for each in ngrams_prob[i+2]:\n",
        "        if each[0][:-1] == ngram_2[i+1]:\n",
        "            count += 1\n",
        "            pred_2[i+1].append(each[0][-1])\n",
        "            if count == 5:\n",
        "                break\n",
        "    if count < 5:\n",
        "        while count != 5:\n",
        "            pred_2[i+1].append(\"NOT FOUND\")\n",
        "            count += 1\n",
        "\n",
        "# Display predictions\n",
        "print(f\"String 1: '{str1}'\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Bigram predictions: {pred_1[1]}\")\n",
        "print(f\"Trigram predictions: {pred_1[2]}\")\n",
        "print(f\"Fourgram predictions: {pred_1[3]}\")\n",
        "\n",
        "print(f\"\\nString 2: '{str2}'\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Bigram predictions: {pred_2[1]}\")\n",
        "print(f\"Trigram predictions: {pred_2[2]}\")\n",
        "print(f\"Fourgram predictions: {pred_2[3]}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nâœ… N-gram language model implementation complete!\")\n"
      ],
      "metadata": {
        "id": "3d07ps7UFI-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a158dd-0328-42c2-e676-6724c37b5960"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CORPUS STATISTICS\n",
            "============================================================\n",
            "The number of sentences is 6\n",
            "The number of tokens is 284\n",
            "The average number of tokens per sentence is 47\n",
            "The number of unique tokens are 137\n",
            "============================================================\n",
            "\n",
            "BUILDING N-GRAM MODELS...\n",
            "============================================================\n",
            "\n",
            "Most common n-grams without stopword removal and without add-1 smoothing:\n",
            "============================================================\n",
            "Most common bigrams: [(('her', 'sister'), 2), (('pictures', 'or'), 2), (('or', 'conversations'), 2), (('rabbit', 'with'), 2), (('the', 'rabbit'), 2)]\n",
            "\n",
            "Most common trigrams: [(('pictures', 'or', 'conversations'), 2), (('alice', 'was', 'beginning'), 1), (('was', 'beginning', 'to'), 1), (('beginning', 'to', 'get'), 1), (('to', 'get', 'very'), 1)]\n",
            "\n",
            "Most common fourgrams: [(('alice', 'was', 'beginning', 'to'), 1), (('was', 'beginning', 'to', 'get'), 1), (('beginning', 'to', 'get', 'very'), 1), (('to', 'get', 'very', 'tired'), 1), (('get', 'very', 'tired', 'of'), 1)]\n",
            "============================================================\n",
            "\n",
            "\n",
            "Most common n-grams with stopword removal and without add-1 smoothing:\n",
            "============================================================\n",
            "Most common unigrams: [('rabbit', 5), ('alice', 4), ('sister', 2), ('nothing', 2), ('book', 2), ('pictures', 2), ('conversations', 2), ('thought', 2), ('mind', 2), ('ran', 2)]\n",
            "\n",
            "Most common bigrams: [(('pictures', 'conversations'), 2), (('oh', 'dear'), 2), (('waistcoat', 'pocket'), 2), (('alice', 'beginning'), 1), (('beginning', 'get'), 1), (('get', 'tired'), 1), (('tired', 'sitting'), 1), (('sitting', 'sister'), 1), (('sister', 'bank'), 1), (('bank', 'nothing'), 1)]\n",
            "============================================================\n",
            "\n",
            "\n",
            "APPLYING ADD-1 SMOOTHING...\n",
            "============================================================\n",
            "\n",
            "Most common n-grams without stopword removal and with add-1 smoothing:\n",
            "============================================================\n",
            "Most common unigrams: [[('the',), 0.03553299492385787], [('it',), 0.030456852791878174], [('to',), 0.025380710659898477], [('a',), 0.02284263959390863], [('of',), 0.02284263959390863], [('and',), 0.02284263959390863], [('her',), 0.02284263959390863], [('she',), 0.02030456852791878], [('rabbit',), 0.015228426395939087], [('was',), 0.015228426395939087]]\n",
            "\n",
            "Most common bigrams: [[('it', 'and'), 0.010330578512396695], [('out', 'of'), 0.008264462809917356], [('rabbit', 'with'), 0.006198347107438017], [('a', 'watch'), 0.006198347107438017], [('waistcoat', 'pocket'), 0.006198347107438017], [('the', 'rabbit'), 0.006198347107438017], [('to', 'her'), 0.006198347107438017], [('her', 'sister'), 0.006198347107438017], [('by', 'her'), 0.006198347107438017], [('or', 'conversations'), 0.006198347107438017]]\n",
            "\n",
            "Most common trigrams: [[('pictures', 'or', 'conversations'), 0.006134969325153374], [('could', 'for', 'the'), 0.00408997955010225], [('to', 'have', 'wondered'), 0.00408997955010225], [('out', 'of', 'it'), 0.00408997955010225], [('feel', 'very', 'sleepy'), 0.00408997955010225], [('book', 'her', 'sister'), 0.00408997955010225], [('or', 'conversations', 'in'), 0.00408997955010225], [('that', 'nor', 'did'), 0.00408997955010225], [('and', 'then', 'hurried'), 0.00408997955010225], [('sister', 'on', 'the'), 0.00408997955010225]]\n",
            "\n",
            "Most common fourgrams: [[('she', 'was', 'considering', 'in'), 0.004166666666666667], [('the', 'trouble', 'of', 'getting'), 0.004166666666666667], [('she', 'had', 'never', 'before'), 0.004166666666666667], [('that', 'she', 'ought', 'to'), 0.004166666666666667], [('never', 'before', 'seen', 'a'), 0.004166666666666667], [('on', 'the', 'bank', 'and'), 0.004166666666666667], [('day', 'made', 'her', 'feel'), 0.004166666666666667], [('trouble', 'of', 'getting', 'up'), 0.004166666666666667], [('and', 'picking', 'the', 'daisies'), 0.004166666666666667], [('i', 'shall', 'be', 'late'), 0.004166666666666667]]\n",
            "============================================================\n",
            "\n",
            "\n",
            "NEXT WORD PREDICTION\n",
            "============================================================\n",
            "String 1 n-grams: {1: ('very',), 2: ('was', 'very'), 3: ('alice', 'was', 'very')}\n",
            "String 2 n-grams: {1: ('the',), 2: ('across', 'the'), 3: ('ran', 'across', 'the')}\n",
            "\n",
            "String 1: 'alice was very'\n",
            "============================================================\n",
            "Bigram predictions: ['tired', 'sleepy', 'remarkable', 'much', 'NOT FOUND']\n",
            "Trigram predictions: ['NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND']\n",
            "Fourgram predictions: ['NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND']\n",
            "\n",
            "String 2: 'she ran across the'\n",
            "============================================================\n",
            "Bigram predictions: ['rabbit', 'field', 'way', 'use', 'book']\n",
            "Trigram predictions: ['field', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND']\n",
            "Fourgram predictions: ['field', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND']\n",
            "============================================================\n",
            "\n",
            "âœ… N-gram language model implementation complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "kbpqCBaLzVSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§® **Lab Assignment**\n",
        "\n",
        "### **Instructions:**\n",
        "\n",
        "1. Modify the codes above to use WSJ corpus as the training  corpus for the n-gram language model.  The WSJ corpus can be can be downloaded from here: https://drive.google.com/file/d/1JMdeLEMC6Zi1XONuDUIYBPJi2M53mcUp/view?usp=drive_link\n",
        "\n",
        "2. By using *at least 5 incomplete phrases*, evaluate the model for next word prediction.\n"
      ],
      "metadata": {
        "id": "cFGaPinHQQ53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# ========================================\n",
        "# STEP 1: CLONE REPO AND LOAD WSJ CORPUS\n",
        "# ========================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CLONING GITHUB REPOSITORY...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "!git clone https://github.com/kedrick07/NLP-Semester-5.git\n",
        "\n",
        "%cd NLP-Semester-5\n",
        "\n",
        "print(\"\\nâœ… Repository cloned successfully!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Read WSJ corpus from your repo\n",
        "with open('WSJ Text.txt', 'r', encoding='utf-8') as file:\n",
        "    corpus = file.read()\n",
        "\n",
        "print(\"\\nâœ… WSJ Corpus loaded from GitHub!\")\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# STEP 2: PREPROCESS WSJ CORPUS\n",
        "# ========================================\n",
        "\n",
        "# Clean text but preserve spaces\n",
        "corpus = corpus.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "corpus = re.sub(r'\\s+', ' ', corpus)  # Remove extra whitespace\n",
        "\n",
        "print(\"\\nCORPUS STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tokenize into sentences first\n",
        "sents = nltk.sent_tokenize(corpus)\n",
        "print(f\"Number of sentences: {len(sents)}\")\n",
        "\n",
        "# Tokenize into words\n",
        "words = nltk.word_tokenize(corpus)\n",
        "print(f\"Number of tokens: {len(words)}\")\n",
        "\n",
        "average_tokens = round(len(words)/len(sents)) if len(sents) > 0 else 0\n",
        "print(f\"Average tokens per sentence: {average_tokens}\")\n",
        "\n",
        "unique_tokens = set(words)\n",
        "print(f\"Number of unique tokens: {len(unique_tokens)}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# STEP 3: BUILD N-GRAM MODELS\n",
        "# ========================================\n",
        "\n",
        "print(\"\\nBUILDING N-GRAM MODELS...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokenized_text = []\n",
        "\n",
        "for sentence in sents:\n",
        "    sentence = sentence.lower()\n",
        "    sequence = word_tokenize(sentence)\n",
        "    # Remove only punctuation, keep words\n",
        "    sequence = [word for word in sequence if word.isalnum()]\n",
        "    if len(sequence) > 0:\n",
        "        tokenized_text.append(sequence)\n",
        "\n",
        "# Build n-grams\n",
        "ngrams_all = {1: [], 2: [], 3: [], 4: []}\n",
        "for sentence in tokenized_text:\n",
        "    for i in range(4):\n",
        "        ngrams_all[i+1].extend(list(ngrams(sentence, i+1)))\n",
        "\n",
        "print(f\"Total unigrams: {len(ngrams_all[1])}\")\n",
        "print(f\"Total bigrams: {len(ngrams_all[2])}\")\n",
        "print(f\"Total trigrams: {len(ngrams_all[3])}\")\n",
        "print(f\"Total fourgrams: {len(ngrams_all[4])}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# STEP 4: APPLY ADD-1 SMOOTHING\n",
        "# ========================================\n",
        "\n",
        "print(\"\\nAPPLYING ADD-1 SMOOTHING...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get vocabulary for each n-gram order\n",
        "ngrams_voc = {1: set([]), 2: set([]), 3: set([]), 4: set([])}\n",
        "for i in range(4):\n",
        "    for gram in ngrams_all[i+1]:\n",
        "        ngrams_voc[i+1].add(gram)\n",
        "\n",
        "# Count total n-grams and vocabulary size\n",
        "total_ngrams = {1: 0, 2: 0, 3: 0, 4: 0}\n",
        "total_voc = {1: 0, 2: 0, 3: 0, 4: 0}\n",
        "for i in range(4):\n",
        "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
        "    total_voc[i+1] = len(ngrams_voc[i+1])\n",
        "\n",
        "print(f\"Vocabulary sizes:\")\n",
        "print(f\"  Unigram: {total_voc[1]}\")\n",
        "print(f\"  Bigram: {total_voc[2]}\")\n",
        "print(f\"  Trigram: {total_voc[3]}\")\n",
        "print(f\"  Fourgram: {total_voc[4]}\")\n",
        "\n",
        "# Calculate smoothed probabilities\n",
        "ngrams_prob = {1: {}, 2: {}, 3: {}, 4: {}}\n",
        "for i in range(4):\n",
        "    for ngram in ngrams_voc[i+1]:\n",
        "        count = ngrams_all[i+1].count(ngram)\n",
        "        # Add-1 smoothing\n",
        "        prob = (count + 1) / (total_ngrams[i+1] + total_voc[i+1])\n",
        "        ngrams_prob[i+1][ngram] = prob\n",
        "\n",
        "# Sort by probability\n",
        "ngrams_prob_sorted = {1: [], 2: [], 3: [], 4: []}\n",
        "for i in range(4):\n",
        "    ngrams_prob_sorted[i+1] = sorted(ngrams_prob[i+1].items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"âœ… Add-1 smoothing applied successfully!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# STEP 5: NEXT WORD PREDICTION (5 TEST PHRASES)\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\\nNEXT WORD PREDICTION - EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_phrases = [\n",
        "    'the company said',\n",
        "    'stock market is',\n",
        "    'according to the',\n",
        "    'new york times',\n",
        "    'in the first'\n",
        "]\n",
        "\n",
        "def predict_next_words(phrase, ngrams_prob, top_n=5):\n",
        "    tokens = word_tokenize(phrase.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "    predictions = {1: [], 2: [], 3: []}\n",
        "\n",
        "    # Bigram prediction (last 1 word)\n",
        "    if len(tokens) >= 1:\n",
        "        context = (tokens[-1],)\n",
        "        for ngram, prob in ngrams_prob[2]:\n",
        "            if ngram[:-1] == context:\n",
        "                predictions[1].append((ngram[-1], prob))\n",
        "                if len(predictions[1]) >= top_n:\n",
        "                    break\n",
        "\n",
        "    # Trigram prediction (last 2 words)\n",
        "    if len(tokens) >= 2:\n",
        "        context = tuple(tokens[-2:])\n",
        "        for ngram, prob in ngrams_prob[3]:\n",
        "            if ngram[:-1] == context:\n",
        "                predictions[2].append((ngram[-1], prob))\n",
        "                if len(predictions[2]) >= top_n:\n",
        "                    break\n",
        "\n",
        "    # Fourgram prediction (last 3 words)\n",
        "    if len(tokens) >= 3:\n",
        "        context = tuple(tokens[-3:])\n",
        "        for ngram, prob in ngrams_prob[4]:\n",
        "            if ngram[:-1] == context:\n",
        "                predictions[3].append((ngram[-1], prob))\n",
        "                if len(predictions[3]) >= top_n:\n",
        "                    break\n",
        "\n",
        "    # Fill with \"NOT FOUND\" if needed\n",
        "    for i in range(1, 4):\n",
        "        while len(predictions[i]) < top_n:\n",
        "            predictions[i].append((\"NOT FOUND\", 0.0))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Test each phrase\n",
        "for idx, phrase in enumerate(test_phrases, 1):\n",
        "    print(f\"\\n{idx}. Test Phrase: '{phrase}'\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    predictions = predict_next_words(phrase, ngrams_prob_sorted, top_n=5)\n",
        "\n",
        "    print(f\"Bigram predictions (top 5):\")\n",
        "    for word, prob in predictions[1]:\n",
        "        print(f\"  - {word:15} (probability: {prob:.8f})\")\n",
        "\n",
        "    print(f\"\\nTrigram predictions (top 5):\")\n",
        "    for word, prob in predictions[2]:\n",
        "        print(f\"  - {word:15} (probability: {prob:.8f})\")\n",
        "\n",
        "    print(f\"\\nFourgram predictions (top 5):\")\n",
        "    for word, prob in predictions[3]:\n",
        "        print(f\"  - {word:15} (probability: {prob:.8f})\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nâœ… N-gram language model evaluation complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# DISPLAY MODEL SUMMARY\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\\nMODEL STATISTICS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training corpus: WSJ Text.txt (from GitHub)\")\n",
        "print(f\"Total sentences: {len(sents)}\")\n",
        "print(f\"Total tokens: {len(words)}\")\n",
        "print(f\"Unique tokens: {len(unique_tokens)}\")\n",
        "print(f\"N-gram model orders: Unigram, Bigram, Trigram, Fourgram\")\n",
        "print(f\"Smoothing technique: Add-1 (Laplace) Smoothing\")\n",
        "print(f\"Test phrases evaluated: {len(test_phrases)}\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "2yDO-MiYkpho",
        "outputId": "836eb4b6-a230-4e2d-bb4b-5ca898950c52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CLONING GITHUB REPOSITORY...\n",
            "============================================================\n",
            "Cloning into 'NLP-Semester-5'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 91 (delta 43), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (91/91), 487.59 KiB | 4.83 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "/content/NLP-Semester-5\n",
            "\n",
            "âœ… Repository cloned successfully!\n",
            "============================================================\n",
            "\n",
            "âœ… WSJ Corpus loaded from GitHub!\n",
            "\n",
            "CORPUS STATISTICS\n",
            "============================================================\n",
            "Number of sentences: 8933\n",
            "Number of tokens: 212002\n",
            "Average tokens per sentence: 24\n",
            "Number of unique tokens: 19109\n",
            "============================================================\n",
            "\n",
            "BUILDING N-GRAM MODELS...\n",
            "============================================================\n",
            "Total unigrams: 173450\n",
            "Total bigrams: 164547\n",
            "Total trigrams: 155675\n",
            "Total fourgrams: 146856\n",
            "============================================================\n",
            "\n",
            "APPLYING ADD-1 SMOOTHING...\n",
            "============================================================\n",
            "Vocabulary sizes:\n",
            "  Unigram: 14244\n",
            "  Bigram: 91464\n",
            "  Trigram: 135629\n",
            "  Fourgram: 140505\n",
            "âœ… Add-1 smoothing applied successfully!\n",
            "============================================================\n",
            "\n",
            "\n",
            "NEXT WORD PREDICTION - EVALUATION\n",
            "============================================================\n",
            "\n",
            "1. Test Phrase: 'the company said'\n",
            "------------------------------------------------------------\n",
            "Bigram predictions (top 5):\n",
            "  - it              (probability: 0.00067966)\n",
            "  - the             (probability: 0.00063669)\n",
            "  - that            (probability: 0.00024218)\n",
            "  - he              (probability: 0.00016796)\n",
            "  - they            (probability: 0.00015234)\n",
            "\n",
            "Trigram predictions (top 5):\n",
            "  - it              (probability: 0.00004463)\n",
            "  - the             (probability: 0.00003433)\n",
            "  - its             (probability: 0.00001373)\n",
            "  - earnings        (probability: 0.00001373)\n",
            "  - that            (probability: 0.00001030)\n",
            "\n",
            "Fourgram predictions (top 5):\n",
            "  - it              (probability: 0.00003828)\n",
            "  - the             (probability: 0.00002784)\n",
            "  - its             (probability: 0.00001392)\n",
            "  - that            (probability: 0.00001044)\n",
            "  - earnings        (probability: 0.00001044)\n",
            "============================================================\n",
            "\n",
            "2. Test Phrase: 'stock market is'\n",
            "------------------------------------------------------------\n",
            "Bigram predictions (top 5):\n",
            "  - a               (probability: 0.00042967)\n",
            "  - the             (probability: 0.00029296)\n",
            "  - that            (probability: 0.00015624)\n",
            "  - to              (probability: 0.00015234)\n",
            "  - expected        (probability: 0.00014843)\n",
            "\n",
            "Trigram predictions (top 5):\n",
            "  - still           (probability: 0.00000687)\n",
            "  - sending         (probability: 0.00000687)\n",
            "  - becoming        (probability: 0.00000687)\n",
            "  - sick            (probability: 0.00000687)\n",
            "  - beginning       (probability: 0.00000687)\n",
            "\n",
            "Fourgram predictions (top 5):\n",
            "  - sick            (probability: 0.00000696)\n",
            "  - NOT FOUND       (probability: 0.00000000)\n",
            "  - NOT FOUND       (probability: 0.00000000)\n",
            "  - NOT FOUND       (probability: 0.00000000)\n",
            "  - NOT FOUND       (probability: 0.00000000)\n",
            "============================================================\n",
            "\n",
            "3. Test Phrase: 'according to the'\n",
            "------------------------------------------------------------\n",
            "Bigram predictions (top 5):\n",
            "  - company         (probability: 0.00126557)\n",
            "  - new             (probability: 0.00053513)\n",
            "  - first           (probability: 0.00048435)\n",
            "  - market          (probability: 0.00048045)\n",
            "  - stock           (probability: 0.00030858)\n",
            "\n",
            "Trigram predictions (top 5):\n",
            "  - company         (probability: 0.00003776)\n",
            "  - bay             (probability: 0.00003090)\n",
            "  - extent          (probability: 0.00002060)\n",
            "  - new             (probability: 0.00002060)\n",
            "  - stock           (probability: 0.00001716)\n",
            "\n",
            "Fourgram predictions (top 5):\n",
            "  - officials       (probability: 0.00000696)\n",
            "  - suit            (probability: 0.00000696)\n",
            "  - lead            (probability: 0.00000696)\n",
            "  - british         (probability: 0.00000696)\n",
            "  - attorney        (probability: 0.00000696)\n",
            "============================================================\n",
            "\n",
            "4. Test Phrase: 'new york times'\n",
            "------------------------------------------------------------\n",
            "Bigram predictions (top 5):\n",
            "  - as              (probability: 0.00002734)\n",
            "  - the             (probability: 0.00002734)\n",
            "  - and             (probability: 0.00001953)\n",
            "  - index           (probability: 0.00001562)\n",
            "  - earnings        (probability: 0.00001562)\n",
            "\n",
            "Trigram predictions (top 5):\n",
            "  - earnings        (probability: 0.00000687)\n",
            "  - closed          (probability: 0.00000687)\n",
            "  - the             (probability: 0.00000687)\n",
            "  - chairman        (probability: 0.00000687)\n",
            "  - NOT FOUND       (probability: 0.00000000)\n",
            "\n",
            "Fourgram predictions (top 5):\n",
            "  - the             (probability: 0.00000696)\n",
            "  - chairman        (probability: 0.00000696)\n",
            "  - earnings        (probability: 0.00000696)\n",
            "  - closed          (probability: 0.00000696)\n",
            "  - NOT FOUND       (probability: 0.00000000)\n",
            "============================================================\n",
            "\n",
            "5. Test Phrase: 'in the first'\n",
            "------------------------------------------------------------\n",
            "Bigram predictions (top 5):\n",
            "  - time            (probability: 0.00008984)\n",
            "  - quarter         (probability: 0.00005859)\n",
            "  - half            (probability: 0.00005078)\n",
            "  - boston          (probability: 0.00003906)\n",
            "  - nine            (probability: 0.00003906)\n",
            "\n",
            "Trigram predictions (top 5):\n",
            "  - time            (probability: 0.00007896)\n",
            "  - nine            (probability: 0.00003433)\n",
            "  - half            (probability: 0.00003433)\n",
            "  - six             (probability: 0.00002403)\n",
            "  - section         (probability: 0.00002060)\n",
            "\n",
            "Fourgram predictions (top 5):\n",
            "  - half            (probability: 0.00002436)\n",
            "  - nine            (probability: 0.00001392)\n",
            "  - section         (probability: 0.00001392)\n",
            "  - 24              (probability: 0.00001044)\n",
            "  - few             (probability: 0.00001044)\n",
            "============================================================\n",
            "\n",
            "âœ… N-gram language model evaluation complete!\n",
            "============================================================\n",
            "\n",
            "\n",
            "MODEL STATISTICS SUMMARY\n",
            "============================================================\n",
            "Training corpus: WSJ Text.txt (from GitHub)\n",
            "Total sentences: 8933\n",
            "Total tokens: 212002\n",
            "Unique tokens: 19109\n",
            "N-gram model orders: Unigram, Bigram, Trigram, Fourgram\n",
            "Smoothing technique: Add-1 (Laplace) Smoothing\n",
            "Test phrases evaluated: 5\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ðŸ“¤ **Submission Reminder:**  \n",
        "\n",
        "Submit the following via PutraBlast:\n",
        "\n",
        "- The link to your Colab notebook (as *Viewer link*). Ensure the file name is `LabNo_StudentID_Name.ipynb`.\n",
        "- Output of next word predictions for the 5 strings using the probability models of bigrams, trigrams, and fourgrams, in a text file.\n"
      ],
      "metadata": {
        "id": "_xIrY2pJe_kT"
      }
    }
  ]
}