{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kedrick07/NLP-Semester-5/blob/main/Lab_3_224232_Kedrick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ðŸ§  Warm-up Examples**"
      ],
      "metadata": {
        "id": "sqas9Ip0_-Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 1: Reading a Text File**"
      ],
      "metadata": {
        "id": "uT135IIYied0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload and read a text file\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:500])  # Show the first 500 characters\n"
      ],
      "metadata": {
        "id": "RbKFInaz70Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: What does this code do? Why do we use \"utf-8\" encoding?\n",
        "\n",
        "---\n",
        "\n",
        "- import files from google colab\n",
        "- encode file with utf-8\n",
        "- utf-8 encodes every Unicode char while remaining fully compatible with the first 128 ASCII chars, so ASCII only files are byte to byte same and render cleanly without a need of special handling"
      ],
      "metadata": {
        "id": "nM94F-l6hPvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 2: Sentence and Word Tokenisation**"
      ],
      "metadata": {
        "id": "-yZKjbNbhe5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(\"Number of sentences:\", len(sentences))\n",
        "print(\"First sentence:\", sentences[0])\n",
        "\n",
        "tokens = nltk.word_tokenize(sentences[0])\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "68SlfK3jh0VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: What is the difference between `sent_tokenize` and `word_tokenize`?\n",
        "\n",
        "---\n",
        "- (sent_tokenize) uses Punkt sentence tokenizer which is a learned rule-based model that looks at punctiation and context to decide the limitations to reduce the false splitting\n",
        "- (word_tokenize) use Treebank tokenizer that seprates words and punctuation into distinct tokens"
      ],
      "metadata": {
        "id": "S3eR_W3DiDcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 3: POS Tagging with Default (Penn Treebank) Tagset**\n",
        "\n",
        "This is the Penn Treebank tagset â€” more detailed, but harder to memorise."
      ],
      "metadata": {
        "id": "w6Df68Iuial0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "print(tagged)"
      ],
      "metadata": {
        "id": "-vmpckX1juwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: Notice tags like NN, VBZ, DT, JJ.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_q4uaDXfjyk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 4: POS Tagging with the Universal Tagset**"
      ],
      "metadata": {
        "id": "MqdZfV8kj_sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_universal = nltk.pos_tag(tokens, tagset='universal')\n",
        "print(tagged_universal)"
      ],
      "metadata": {
        "id": "92PVpEtNkLpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: Universal tagset uses simpler tags (e.g. NOUN, VERB, DET, ADJ, ADV).\n",
        "Refer to https://universaldependencies.org/u/pos/\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CMFaPP8EkVQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 5: Counting POS Frequencies**"
      ],
      "metadata": {
        "id": "soorIoECkaeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "pos_counts = Counter(tag for (word, tag) in tagged_universal)\n",
        "print(pos_counts)"
      ],
      "metadata": {
        "id": "7t3t2p_SkfWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: How could frequency analysis help in understanding the nature of the text (e.g. news vs fiction)?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vtgR-hqskgRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN THIS LINE FIRST TO CLONE GITHUB REPO"
      ],
      "metadata": {
        "id": "1KA-rFod3RBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/kedrick07/NLP-Semester-5.git\"\n",
        "REPO_NAME = \"NLP-Semester-5\"\n",
        "BRANCH = \"main\"  # change if your branch is not 'main'\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"ðŸ”„ Cloning repo: {REPO_URL}\")\n",
        "    !git clone -b {BRANCH} {REPO_URL}\n",
        "else:\n",
        "    print(f\"âœ… Repo '{REPO_NAME}' already exists, updating...\")\n",
        "    %cd {REPO_NAME}\n",
        "    !git fetch origin\n",
        "    !git stash\n",
        "    !git pull origin {BRANCH}\n",
        "    !git stash pop || echo \"No stashed changes to pop\"\n",
        "\n",
        "    # Optional: if youâ€™ve made edits in Colab and want to push them back\n",
        "    commit_msg = \"Auto update from Colab\"\n",
        "    !git add .\n",
        "    !git diff-index --quiet HEAD || (git commit -m \"{commit_msg}\" && git push origin {BRANCH})\n",
        "    %cd ..\n"
      ],
      "metadata": {
        "id": "X10Jhqau05ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**âœï¸ Exercises**\n"
      ],
      "metadata": {
        "id": "RyffPUmfk1Ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1: File Reading and Sentence Counting**\n",
        "\n",
        "Upload your own short text file (about 3â€“5 sentences). Write code to:\n",
        "*  read the file\n",
        "*  count and print how many sentences are in the file\n",
        "*  print only the second sentence"
      ],
      "metadata": {
        "id": "YJvIyVsqlFTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "obPiHE3lFREr",
        "outputId": "4a42c760-1806-4092-faf2-205bf837d1a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m245.8/253.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from docx import Document\n",
        "\n",
        "# Download the new punkt_tab tokenizer\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Read the .docx file\n",
        "filename = \"NLP-Semester-5/lab3_exercise1.docx\"\n",
        "doc = Document(filename)\n",
        "\n",
        "# Extract all text from paragraphs\n",
        "text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "# Count and print number of sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(\"Number of sentences:\", len(sentences))\n",
        "\n",
        "# Print only the second sentence\n",
        "if len(sentences) >= 2:\n",
        "    print(\"Second sentence:\", sentences[1])\n",
        "else:\n",
        "    print(\"The file has fewer than 2 sentences.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Kp0UZ4jclxim",
        "outputId": "1ee1d966-2e25-4c1d-9384-456dead311f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 5\n",
            "Second sentence: A single red leaf floated across the pond, spinning slowly before sinking beneath the surface.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2: POS Tagging Practice**\n",
        "\n",
        "Use the sentence below.\n",
        "Tokenize it and print the POS tags using the universal tagset."
      ],
      "metadata": {
        "id": "1PEr5Trfl0Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download required packages for newer NLTK versions\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # Updated tagger name\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "sentence = \"Artificial intelligence transforms modern education.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# POS tag with universal tagset\n",
        "pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
        "\n",
        "# Print the POS tags\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "DH1ps1khmEgd",
        "outputId": "5d526fa1-6c23-4e06-9bec-3070104c926c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Artificial', 'ADJ'), ('intelligence', 'NOUN'), ('transforms', 'NOUN'), ('modern', 'ADJ'), ('education', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3: Count Only NOUNs and VERBs**\n",
        "\n",
        "Modify your code to count how many tokens are tagged as NOUN or VERB."
      ],
      "metadata": {
        "id": "Ej5wabnUmKBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download required packages\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "sentence = \"Artificial intelligence transforms modern education.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# POS tag with universal tagset\n",
        "pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
        "\n",
        "# Print all POS tags\n",
        "print(\"All POS tags:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# Count only NOUNs and VERBs\n",
        "noun_count = 0\n",
        "verb_count = 0\n",
        "\n",
        "for word, tag in pos_tags:\n",
        "    if tag == 'NOUN':\n",
        "        noun_count += 1\n",
        "    elif tag == 'VERB':\n",
        "        verb_count += 1\n",
        "\n",
        "print(\"\\nNOUN count:\", noun_count)\n",
        "print(\"VERB count:\", verb_count)\n",
        "print(\"Total NOUNs and VERBs:\", noun_count + verb_count)\n"
      ],
      "metadata": {
        "id": "fpVR0swVmXOF",
        "outputId": "b67465f9-33b1-49f8-aee9-7c68568ea7c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All POS tags:\n",
            "[('Artificial', 'ADJ'), ('intelligence', 'NOUN'), ('transforms', 'NOUN'), ('modern', 'ADJ'), ('education', 'NOUN'), ('.', '.')]\n",
            "\n",
            "NOUN count: 3\n",
            "VERB count: 0\n",
            "Total NOUNs and VERBs: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§® **Lab Assignment**\n",
        "Complete the following tasks and ensure your answers run without error.\n"
      ],
      "metadata": {
        "id": "cFGaPinHQQ53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **Step 1: Manual Tagging**\n",
        "\n",
        "Go to an English newspaper website (e.g. The Star, New Straits Times).\n",
        "\n",
        "Copy one complete article into a `.txt` file.\n",
        "\n",
        "Select the first sentence and manually tag each word using the UD POS tags (ADJ, NOUN, VERB, etc.). Reference: https://universaldependencies.org/u/pos/\n",
        "\n",
        "Which words were difficult to tag? Why?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "z4XVzjcUiWmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_correction(input_word, dictionary):\n",
        "  #Write your code here"
      ],
      "metadata": {
        "id": "ZcVriQMoMY1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "###**Step 2: Automatic Tagging with NLTK**\n",
        "\n",
        "Using what you learned in above sections, write Python code to:\n",
        "\n",
        "1. Read the article text file from Step 1.\n",
        "2. Tokenize the text.\n",
        "3. Apply POS tagging using NLTK with the Universal tagset.\n",
        "4. Save all tagged tokens into an output text file (`tagged_output.txt`).\n",
        "5. Calculate and print the frequency of each POS tag. Use `Counter()` to count POS tags.\n",
        "\n",
        "Make sure you download required NLTK resources (`punkt`, `averaged_perceptron_tagger`)."
      ],
      "metadata": {
        "id": "IrdkjD0yjCNb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yDO-MiYkpho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ðŸ“¤ **Submission Reminder:**  \n",
        "\n",
        "Submit the following via PutraBlast:\n",
        "\n",
        "- The link to your Colab notebook (as *Viewer link*). Ensure the file name is `LabNo_StudentID_Name.ipynb`.\n",
        "- The article text file (`.txt`)\n",
        "- The tagged output file (`tagged_output.txt`)\n",
        "- Screenshot of your manual tagging  \n",
        "---\n"
      ],
      "metadata": {
        "id": "_xIrY2pJe_kT"
      }
    }
  ]
}