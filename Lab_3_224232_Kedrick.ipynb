{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kedrick07/NLP-Semester-5/blob/main/Lab_3_224232_Kedrick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ðŸ§  Warm-up Examples**"
      ],
      "metadata": {
        "id": "sqas9Ip0_-Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 1: Reading a Text File**"
      ],
      "metadata": {
        "id": "uT135IIYied0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload and read a text file\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:500])  # Show the first 500 characters\n"
      ],
      "metadata": {
        "id": "RbKFInaz70Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: What does this code do? Why do we use \"utf-8\" encoding?\n",
        "\n",
        "---\n",
        "\n",
        "- import files from google colab\n",
        "- encode file with utf-8\n",
        "- utf-8 encodes every Unicode char while remaining fully compatible with the first 128 ASCII chars, so ASCII only files are byte to byte same and render cleanly without a need of special handling"
      ],
      "metadata": {
        "id": "nM94F-l6hPvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 2: Sentence and Word Tokenisation**"
      ],
      "metadata": {
        "id": "-yZKjbNbhe5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(\"Number of sentences:\", len(sentences))\n",
        "print(\"First sentence:\", sentences[0])\n",
        "\n",
        "tokens = nltk.word_tokenize(sentences[0])\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "68SlfK3jh0VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: What is the difference between `sent_tokenize` and `word_tokenize`?\n",
        "\n",
        "---\n",
        "- (sent_tokenize) uses Punkt sentence tokenizer which is a learned rule-based model that looks at punctiation and context to decide the limitations to reduce the false splitting\n",
        "- (word_tokenize) use Treebank tokenizer that seprates words and punctuation into distinct tokens"
      ],
      "metadata": {
        "id": "S3eR_W3DiDcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 3: POS Tagging with Default (Penn Treebank) Tagset**\n",
        "\n",
        "This is the Penn Treebank tagset â€” more detailed, but harder to memorise."
      ],
      "metadata": {
        "id": "w6Df68Iuial0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "print(tagged)"
      ],
      "metadata": {
        "id": "-vmpckX1juwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: Notice tags like NN, VBZ, DT, JJ.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_q4uaDXfjyk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 4: POS Tagging with the Universal Tagset**"
      ],
      "metadata": {
        "id": "MqdZfV8kj_sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_universal = nltk.pos_tag(tokens, tagset='universal')\n",
        "print(tagged_universal)"
      ],
      "metadata": {
        "id": "92PVpEtNkLpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: Universal tagset uses simpler tags (e.g. NOUN, VERB, DET, ADJ, ADV).\n",
        "Refer to https://universaldependencies.org/u/pos/\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CMFaPP8EkVQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 5: Counting POS Frequencies**"
      ],
      "metadata": {
        "id": "soorIoECkaeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "pos_counts = Counter(tag for (word, tag) in tagged_universal)\n",
        "print(pos_counts)"
      ],
      "metadata": {
        "id": "7t3t2p_SkfWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: How could frequency analysis help in understanding the nature of the text (e.g. news vs fiction)?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vtgR-hqskgRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN THIS LINE FIRST TO CLONE GITHUB REPO"
      ],
      "metadata": {
        "id": "1KA-rFod3RBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/kedrick07/NLP-Semester-5.git\"\n",
        "REPO_NAME = \"NLP-Semester-5\"\n",
        "BRANCH = \"main\"  # change if your branch is not 'main'\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"ðŸ”„ Cloning repo: {REPO_URL}\")\n",
        "    !git clone -b {BRANCH} {REPO_URL}\n",
        "else:\n",
        "    print(f\"âœ… Repo '{REPO_NAME}' already exists, updating...\")\n",
        "    %cd {REPO_NAME}\n",
        "    !git fetch origin\n",
        "    !git stash\n",
        "    !git pull origin {BRANCH}\n",
        "    !git stash pop || echo \"No stashed changes to pop\"\n",
        "\n",
        "    # Optional: if youâ€™ve made edits in Colab and want to push them back\n",
        "    commit_msg = \"Auto update from Colab\"\n",
        "    !git add .\n",
        "    !git diff-index --quiet HEAD || (git commit -m \"{commit_msg}\" && git push origin {BRANCH})\n",
        "    %cd ..\n"
      ],
      "metadata": {
        "id": "X10Jhqau05ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**âœï¸ Exercises**\n"
      ],
      "metadata": {
        "id": "RyffPUmfk1Ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1: File Reading and Sentence Counting**\n",
        "\n",
        "Upload your own short text file (about 3â€“5 sentences). Write code to:\n",
        "*  read the file\n",
        "*  count and print how many sentences are in the file\n",
        "*  print only the second sentence"
      ],
      "metadata": {
        "id": "YJvIyVsqlFTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "obPiHE3lFREr",
        "outputId": "4a42c760-1806-4092-faf2-205bf837d1a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m245.8/253.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from docx import Document\n",
        "\n",
        "# Download the new punkt_tab tokenizer\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Read the .docx file\n",
        "filename = \"NLP-Semester-5/lab3_exercise1.docx\"\n",
        "doc = Document(filename)\n",
        "\n",
        "# Extract all text from paragraphs\n",
        "text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "# Count and print number of sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(\"Number of sentences:\", len(sentences))\n",
        "\n",
        "# Print only the second sentence\n",
        "if len(sentences) >= 2:\n",
        "    print(\"Second sentence:\", sentences[1])\n",
        "else:\n",
        "    print(\"The file has fewer than 2 sentences.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Kp0UZ4jclxim",
        "outputId": "1ee1d966-2e25-4c1d-9384-456dead311f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 5\n",
            "Second sentence: A single red leaf floated across the pond, spinning slowly before sinking beneath the surface.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2: POS Tagging Practice**\n",
        "\n",
        "Use the sentence below.\n",
        "Tokenize it and print the POS tags using the universal tagset."
      ],
      "metadata": {
        "id": "1PEr5Trfl0Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download required packages for newer NLTK versions\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # Updated tagger name\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "sentence = \"Artificial intelligence transforms modern education.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# POS tag with universal tagset\n",
        "pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
        "\n",
        "# Print the POS tags\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "DH1ps1khmEgd",
        "outputId": "5d526fa1-6c23-4e06-9bec-3070104c926c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Artificial', 'ADJ'), ('intelligence', 'NOUN'), ('transforms', 'NOUN'), ('modern', 'ADJ'), ('education', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3: Count Only NOUNs and VERBs**\n",
        "\n",
        "Modify your code to count how many tokens are tagged as NOUN or VERB."
      ],
      "metadata": {
        "id": "Ej5wabnUmKBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download required packages\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "sentence = \"Artificial intelligence transforms modern education.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# POS tag with universal tagset\n",
        "pos_tags = nltk.pos_tag(tokens, tagset='universal')\n",
        "\n",
        "# Print all POS tags\n",
        "print(\"All POS tags:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# Count only NOUNs and VERBs\n",
        "noun_count = 0\n",
        "verb_count = 0\n",
        "\n",
        "for word, tag in pos_tags:\n",
        "    if tag == 'NOUN':\n",
        "        noun_count += 1\n",
        "    elif tag == 'VERB':\n",
        "        verb_count += 1\n",
        "\n",
        "print(\"\\nNOUN count:\", noun_count)\n",
        "print(\"VERB count:\", verb_count)\n",
        "print(\"Total NOUNs and VERBs:\", noun_count + verb_count)\n"
      ],
      "metadata": {
        "id": "fpVR0swVmXOF",
        "outputId": "b67465f9-33b1-49f8-aee9-7c68568ea7c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All POS tags:\n",
            "[('Artificial', 'ADJ'), ('intelligence', 'NOUN'), ('transforms', 'NOUN'), ('modern', 'ADJ'), ('education', 'NOUN'), ('.', '.')]\n",
            "\n",
            "NOUN count: 3\n",
            "VERB count: 0\n",
            "Total NOUNs and VERBs: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§® **Lab Assignment**\n",
        "Complete the following tasks and ensure your answers run without error.\n"
      ],
      "metadata": {
        "id": "cFGaPinHQQ53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **Step 1: Manual Tagging**\n",
        "\n",
        "Go to an English newspaper website (e.g. The Star, New Straits Times).\n",
        "\n",
        "Copy one complete article into a `.txt` file.\n",
        "\n",
        "Select the first sentence and manually tag each word using the UD POS tags (ADJ, NOUN, VERB, etc.). Reference: https://universaldependencies.org/u/pos/\n",
        "\n",
        "Which words were difficult to tag? Why?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "z4XVzjcUiWmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/kedrick07/NLP-Semester-5.git\n",
        "\n",
        "# Change directory to the cloned repo\n",
        "%cd NLP-Semester-5\n",
        "\n",
        "# Read the article file\n",
        "with open('article_lab3.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"âœ… File loaded successfully!\\n\")\n",
        "print(\"=\"*60)\n",
        "print(text[:300])\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Extract first sentence\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "first_sentence = sentences[0]\n",
        "\n",
        "print(\"\\nðŸ“ FIRST SENTENCE:\")\n",
        "print(\"=\"*60)\n",
        "print(first_sentence)\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Prepare for manual POS tagging\n",
        "words = first_sentence.split()\n",
        "\n",
        "print(\"\\nðŸ·ï¸ MANUAL POS TAGGING:\")\n",
        "print(\"=\"*60)\n",
        "for i, word in enumerate(words, 1):\n",
        "    print(f\"{i:2}. {word:20} / _______\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"UD POS TAGS: ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ,\")\n",
        "print(\"NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, X\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# SPELL CORRECTION FUNCTION\n",
        "# ========================================\n",
        "\n",
        "def suggest_correction(input_word, dictionary):\n",
        "    def levenshtein_distance(word1, word2):\n",
        "        m, n = len(word1), len(word2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "        for i in range(m + 1):\n",
        "            dp[i][0] = i\n",
        "        for j in range(n + 1):\n",
        "            dp[0][j] = j\n",
        "\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if word1[i-1] == word2[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1]\n",
        "                else:\n",
        "                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
        "\n",
        "        return dp[m][n]\n",
        "\n",
        "    distances = [(word, levenshtein_distance(input_word.lower(), word.lower()))\n",
        "                 for word in dictionary]\n",
        "    distances.sort(key=lambda x: x[1])\n",
        "\n",
        "    return [word for word, dist in distances[:3]]\n",
        "\n",
        "\n",
        "# Test spell correction\n",
        "dictionary = [\"spelling\", \"speaking\", \"spell\", \"splitting\", \"hello\",\n",
        "              \"world\", \"python\", \"program\", \"correction\", \"distance\", \"algorithm\"]\n",
        "\n",
        "test_words = [\"speling\", \"wrld\", \"pythom\", \"helo\", \"algoritm\"]\n",
        "\n",
        "print(\"\\n\\nðŸ”¤ SPELL CORRECTION TEST:\")\n",
        "print(\"=\"*60)\n",
        "for word in test_words:\n",
        "    suggestions = suggest_correction(word, dictionary)\n",
        "    print(f\"{word:15} â†’ {suggestions}\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "ZcVriQMoMY1P",
        "outputId": "d88d1cc6-665f-4a01-a3fa-d18c67977580",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-Semester-5'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 85 (delta 39), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (85/85), 482.28 KiB | 3.11 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "/content/NLP-Semester-5/NLP-Semester-5/NLP-Semester-5\n",
            "âœ… File loaded successfully!\n",
            "\n",
            "============================================================\n",
            "KIDNAPPING GANG BUSTED, HANDGUNS, POLICE UNIFORMS SEIZED IN CAMBODIA\n",
            "\n",
            "PHNOM PENH: Eight Cambodian men have been arrested and four handguns\n",
            "seized in connection with impersonating law enforcement officers to\n",
            "kidnap and extort money from multiple foreign nationals in Phnom Penh.\n",
            "\n",
            "The Phnom Penh Police\n",
            "============================================================\n",
            "\n",
            "ðŸ“ FIRST SENTENCE:\n",
            "============================================================\n",
            "KIDNAPPING GANG BUSTED, HANDGUNS, POLICE UNIFORMS SEIZED IN CAMBODIA\n",
            "\n",
            "PHNOM PENH: Eight Cambodian men have been arrested and four handguns\n",
            "seized in connection with impersonating law enforcement officers to\n",
            "kidnap and extort money from multiple foreign nationals in Phnom Penh.\n",
            "============================================================\n",
            "\n",
            "ðŸ·ï¸ MANUAL POS TAGGING:\n",
            "============================================================\n",
            " 1. KIDNAPPING           / _______\n",
            " 2. GANG                 / _______\n",
            " 3. BUSTED,              / _______\n",
            " 4. HANDGUNS,            / _______\n",
            " 5. POLICE               / _______\n",
            " 6. UNIFORMS             / _______\n",
            " 7. SEIZED               / _______\n",
            " 8. IN                   / _______\n",
            " 9. CAMBODIA             / _______\n",
            "10. PHNOM                / _______\n",
            "11. PENH:                / _______\n",
            "12. Eight                / _______\n",
            "13. Cambodian            / _______\n",
            "14. men                  / _______\n",
            "15. have                 / _______\n",
            "16. been                 / _______\n",
            "17. arrested             / _______\n",
            "18. and                  / _______\n",
            "19. four                 / _______\n",
            "20. handguns             / _______\n",
            "21. seized               / _______\n",
            "22. in                   / _______\n",
            "23. connection           / _______\n",
            "24. with                 / _______\n",
            "25. impersonating        / _______\n",
            "26. law                  / _______\n",
            "27. enforcement          / _______\n",
            "28. officers             / _______\n",
            "29. to                   / _______\n",
            "30. kidnap               / _______\n",
            "31. and                  / _______\n",
            "32. extort               / _______\n",
            "33. money                / _______\n",
            "34. from                 / _______\n",
            "35. multiple             / _______\n",
            "36. foreign              / _______\n",
            "37. nationals            / _______\n",
            "38. in                   / _______\n",
            "39. Phnom                / _______\n",
            "40. Penh.                / _______\n",
            "\n",
            "============================================================\n",
            "UD POS TAGS: ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ,\n",
            "NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, X\n",
            "============================================================\n",
            "\n",
            "\n",
            "ðŸ”¤ SPELL CORRECTION TEST:\n",
            "============================================================\n",
            "speling         â†’ ['spelling', 'speaking', 'spell']\n",
            "wrld            â†’ ['world', 'spell', 'hello']\n",
            "pythom          â†’ ['python', 'hello', 'program']\n",
            "helo            â†’ ['hello', 'spell', 'world']\n",
            "algoritm        â†’ ['algorithm', 'world', 'program']\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hard to tag words\n",
        "\n",
        "1. \"have been\" - These are helper verbs (AUX), not the main verb. It was difficult because \"have\" and \"been\" just help the main action word \"arrested\", but they don't show the actual action themselves.\n",
        "\n",
        "2. \"arrested\" and \"seized\" - These words were tricky because they end in -ed and could be either verbs or adjectives. I had to figure out if they showed an action happening or described something.\n",
        "\n",
        "3. \"impersonating\" - Words ending in -ing can be verbs, nouns, or adjectives. Without looking at how it's used in the sentence, it's hard to know which one it is.\n",
        "\n",
        "4. \"to\" (in \"to kidnap\") - The word \"to\" can be different things. Sometimes it's a particle (PART) like in \"to kidnap\", and sometimes it's a preposition (ADP) like in \"go to school\". I had to decide which one based on context.\n",
        "\n",
        "5. \"Phnom Penh\" - This is one city name, but each word needs its own tag (PROPN). It was confusing because it feels like one thing but has to be tagged as two separate words.\n",
        "\n",
        "6. \"law enforcement officers\" - It was hard to tell if \"law\" and \"enforcement\" are adjectives describing \"officers\" or if they are nouns themselves.\n",
        "\n",
        "These words show that POS tagging is difficult because the same word can have different roles depending on how it's used in a sentence.\n"
      ],
      "metadata": {
        "id": "qp56CIwnHOZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "###**Step 2: Automatic Tagging with NLTK**\n",
        "\n",
        "Using what you learned in above sections, write Python code to:\n",
        "\n",
        "1. Read the article text file from Step 1.\n",
        "2. Tokenize the text.\n",
        "3. Apply POS tagging using NLTK with the Universal tagset.\n",
        "4. Save all tagged tokens into an output text file (`tagged_output.txt`).\n",
        "5. Calculate and print the frequency of each POS tag. Use `Counter()` to count POS tags.\n",
        "\n",
        "Make sure you download required NLTK resources (`punkt`, `averaged_perceptron_tagger`)."
      ],
      "metadata": {
        "id": "IrdkjD0yjCNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from google.colab import files\n",
        "\n",
        "# Download required NLTK resources (updated for latest NLTK)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/kedrick07/NLP-Semester-5.git\n",
        "\n",
        "# Change directory to the cloned repo\n",
        "%cd NLP-Semester-5\n",
        "\n",
        "# ========================================\n",
        "# STEP 2: AUTOMATIC POS TAGGING WITH NLTK\n",
        "# ========================================\n",
        "\n",
        "# Read the article file\n",
        "with open('article_lab3.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"âœ… Article loaded\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(f\"Total tokens: {len(tokens)}\\n\")\n",
        "\n",
        "# Apply POS tagging with Universal tagset\n",
        "tagged_tokens = nltk.pos_tag(tokens, tagset='universal')\n",
        "\n",
        "print(\"Sample tagged tokens (first 20):\")\n",
        "print(tagged_tokens[:20])\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Save tagged tokens to output file\n",
        "with open('tagged_output.txt', 'w') as f:\n",
        "    for word, tag in tagged_tokens:\n",
        "        f.write(f\"{word}\\t{tag}\\n\")\n",
        "\n",
        "print(\"\\nâœ… Tagged output saved to 'tagged_output.txt'\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate frequency of each POS tag\n",
        "pos_tags = [tag for word, tag in tagged_tokens]\n",
        "tag_frequency = Counter(pos_tags)\n",
        "\n",
        "print(\"\\nðŸ“Š POS TAG FREQUENCY:\")\n",
        "print(\"=\"*60)\n",
        "for tag, count in tag_frequency.most_common():\n",
        "    print(f\"{tag:10} : {count:5} ({count/len(tokens)*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"\\nTotal unique POS tags: {len(tag_frequency)}\")\n",
        "print(f\"Total tokens tagged: {len(tokens)}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Download the tagged output file\n",
        "print(\"\\nðŸ“¥ Downloading tagged_output.txt...\")\n",
        "files.download('tagged_output.txt')\n",
        "print(\"âœ… Download complete!\")\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# SPELL CORRECTION FUNCTION\n",
        "# ========================================\n",
        "\n",
        "def suggest_correction(input_word, dictionary):\n",
        "    def levenshtein_distance(word1, word2):\n",
        "        m, n = len(word1), len(word2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "        for i in range(m + 1):\n",
        "            dp[i][0] = i\n",
        "        for j in range(n + 1):\n",
        "            dp[0][j] = j\n",
        "\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if word1[i-1] == word2[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1]\n",
        "                else:\n",
        "                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
        "\n",
        "        return dp[m][n]\n",
        "\n",
        "    distances = [(word, levenshtein_distance(input_word.lower(), word.lower()))\n",
        "                 for word in dictionary]\n",
        "    distances.sort(key=lambda x: x[1])\n",
        "\n",
        "    return [word for word, dist in distances[:3]]\n"
      ],
      "metadata": {
        "id": "2yDO-MiYkpho",
        "outputId": "89e82bca-9f00-47f7-9a32-1a822a9c8415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-Semester-5'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 85 (delta 39), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (85/85), 482.28 KiB | 3.26 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "/content/NLP-Semester-5/NLP-Semester-5/NLP-Semester-5/NLP-Semester-5/NLP-Semester-5/NLP-Semester-5\n",
            "âœ… Article loaded\n",
            "\n",
            "============================================================\n",
            "Total tokens: 304\n",
            "\n",
            "Sample tagged tokens (first 20):\n",
            "[('KIDNAPPING', 'NOUN'), ('GANG', 'NOUN'), ('BUSTED', 'NOUN'), (',', '.'), ('HANDGUNS', 'NOUN'), (',', '.'), ('POLICE', 'NOUN'), ('UNIFORMS', 'NOUN'), ('SEIZED', 'NOUN'), ('IN', 'NOUN'), ('CAMBODIA', 'NOUN'), ('PHNOM', 'NOUN'), ('PENH', 'NOUN'), (':', '.'), ('Eight', 'NUM'), ('Cambodian', 'ADJ'), ('men', 'NOUN'), ('have', 'VERB'), ('been', 'VERB'), ('arrested', 'VERB')]\n",
            "\n",
            "============================================================\n",
            "\n",
            "âœ… Tagged output saved to 'tagged_output.txt'\n",
            "\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š POS TAG FREQUENCY:\n",
            "============================================================\n",
            "NOUN       :   111 (36.51%)\n",
            ".          :    37 (12.17%)\n",
            "VERB       :    34 (11.18%)\n",
            "ADP        :    31 (10.20%)\n",
            "DET        :    25 (8.22%)\n",
            "NUM        :    22 (7.24%)\n",
            "ADJ        :    16 (5.26%)\n",
            "CONJ       :     8 (2.63%)\n",
            "PRON       :     8 (2.63%)\n",
            "PRT        :     7 (2.30%)\n",
            "ADV        :     5 (1.64%)\n",
            "\n",
            "============================================================\n",
            "\n",
            "Total unique POS tags: 11\n",
            "Total tokens tagged: 304\n",
            "============================================================\n",
            "\n",
            "ðŸ“¥ Downloading tagged_output.txt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_669cba23-0543-47af-883f-406ebbf8e9f8\", \"tagged_output.txt\", 3015)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Download complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ðŸ“¤ **Submission Reminder:**  \n",
        "\n",
        "Submit the following via PutraBlast:\n",
        "\n",
        "- The link to your Colab notebook (as *Viewer link*). Ensure the file name is `LabNo_StudentID_Name.ipynb`.\n",
        "- The article text file (`.txt`)\n",
        "- The tagged output file (`tagged_output.txt`)\n",
        "- Screenshot of your manual tagging  \n",
        "---\n"
      ],
      "metadata": {
        "id": "_xIrY2pJe_kT"
      }
    }
  ]
}