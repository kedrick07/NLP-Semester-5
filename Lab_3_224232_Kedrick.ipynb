{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kedrick07/NLP-Semester-5/blob/main/Lab_3_224232_Kedrick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ðŸ§  Warm-up Examples**"
      ],
      "metadata": {
        "id": "sqas9Ip0_-Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 1: Reading a Text File**"
      ],
      "metadata": {
        "id": "uT135IIYied0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload and read a text file\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:500])  # Show the first 500 characters\n"
      ],
      "metadata": {
        "id": "RbKFInaz70Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: What does this code do? Why do we use \"utf-8\" encoding?\n",
        "\n",
        "---\n",
        "\n",
        "- import files from google colab\n",
        "- encode file with utf-8\n",
        "- utf-8 encodes every Unicode char while remaining fully compatible with the first 128 ASCII chars, so ASCII only files are byte to byte same and render cleanly without a need of special handling"
      ],
      "metadata": {
        "id": "nM94F-l6hPvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 2: Sentence and Word Tokenisation**"
      ],
      "metadata": {
        "id": "-yZKjbNbhe5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(\"Number of sentences:\", len(sentences))\n",
        "print(\"First sentence:\", sentences[0])\n",
        "\n",
        "tokens = nltk.word_tokenize(sentences[0])\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "68SlfK3jh0VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: What is the difference between `sent_tokenize` and `word_tokenize`?\n",
        "\n",
        "---\n",
        "- (sent_tokenize) uses Punkt sentence tokenizer which is a learned rule-based model that looks at punctiation and context to decide the limitations to reduce the false splitting\n",
        "- (word_tokenize) use Treebank tokenizer that seprates words and punctuation into distinct tokens"
      ],
      "metadata": {
        "id": "S3eR_W3DiDcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 3: POS Tagging with Default (Penn Treebank) Tagset**\n",
        "\n",
        "This is the Penn Treebank tagset â€” more detailed, but harder to memorise."
      ],
      "metadata": {
        "id": "w6Df68Iuial0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "print(tagged)"
      ],
      "metadata": {
        "id": "-vmpckX1juwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: Notice tags like NN, VBZ, DT, JJ.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_q4uaDXfjyk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 4: POS Tagging with the Universal Tagset**"
      ],
      "metadata": {
        "id": "MqdZfV8kj_sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_universal = nltk.pos_tag(tokens, tagset='universal')\n",
        "print(tagged_universal)"
      ],
      "metadata": {
        "id": "92PVpEtNkLpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: Universal tagset uses simpler tags (e.g. NOUN, VERB, DET, ADJ, ADV).\n",
        "Refer to https://universaldependencies.org/u/pos/\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CMFaPP8EkVQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Example 5: Counting POS Frequencies**"
      ],
      "metadata": {
        "id": "soorIoECkaeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "pos_counts = Counter(tag for (word, tag) in tagged_universal)\n",
        "print(pos_counts)"
      ],
      "metadata": {
        "id": "7t3t2p_SkfWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion: How could frequency analysis help in understanding the nature of the text (e.g. news vs fiction)?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vtgR-hqskgRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN THIS LINE FIRST TO CLONE GITHUB REPO"
      ],
      "metadata": {
        "id": "1KA-rFod3RBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/kedrick07/NLP-Semester-5.git\"\n",
        "REPO_NAME = \"NLP-Semester-5\"\n",
        "BRANCH = \"main\"  # change if your branch is not 'main'\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"ðŸ”„ Cloning repo: {REPO_URL}\")\n",
        "    !git clone -b {BRANCH} {REPO_URL}\n",
        "else:\n",
        "    print(f\"âœ… Repo '{REPO_NAME}' already exists, updating...\")\n",
        "    %cd {REPO_NAME}\n",
        "    !git fetch origin\n",
        "    !git stash\n",
        "    !git pull origin {BRANCH}\n",
        "    !git stash pop || echo \"No stashed changes to pop\"\n",
        "\n",
        "    # Optional: if youâ€™ve made edits in Colab and want to push them back\n",
        "    commit_msg = \"Auto update from Colab\"\n",
        "    !git add .\n",
        "    !git diff-index --quiet HEAD || (git commit -m \"{commit_msg}\" && git push origin {BRANCH})\n",
        "    %cd ..\n"
      ],
      "metadata": {
        "id": "X10Jhqau05ho",
        "outputId": "08e73c39-7201-4015-9a2b-e9923202d1b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Repo 'NLP-Semester-5' already exists, updating...\n",
            "/content/NLP-Semester-5\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (3/3), 11.02 KiB | 3.67 MiB/s, done.\n",
            "From https://github.com/kedrick07/NLP-Semester-5\n",
            "   00e4250..a261d80  main       -> origin/main\n",
            "No local changes to save\n",
            "From https://github.com/kedrick07/NLP-Semester-5\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Updating 00e4250..a261d80\n",
            "Fast-forward\n",
            " lab3_exercise1.docx | Bin \u001b[31m0\u001b[m -> \u001b[32m11592\u001b[m bytes\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 lab3_exercise1.docx\n",
            "No stash entries found.\n",
            "No stashed changes to pop\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**âœï¸ Exercises**\n"
      ],
      "metadata": {
        "id": "RyffPUmfk1Ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1: File Reading and Sentence Counting**\n",
        "\n",
        "Upload your own short text file (about 3â€“5 sentences). Write code to:\n",
        "*  read the file\n",
        "*  count and print how many sentences are in the file\n",
        "*  print only the second sentence"
      ],
      "metadata": {
        "id": "YJvIyVsqlFTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#read file from git repo\n",
        "filename = \"NLP-Semester-5/lab3_exercise1.docx\"\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "#count and print no of sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(\"Number of sentences:\", len(sentences))\n",
        "\n",
        "#print only the second chance\n",
        "if len(sentences) >= 2:\n",
        "  print(\"Second sentence:\", sentences[1])\n",
        "else:\n",
        "  print(\"File does not contain enough sentences.\")"
      ],
      "metadata": {
        "id": "Kp0UZ4jclxim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2: POS Tagging Practice**\n",
        "\n",
        "Use the sentence below.\n",
        "Tokenize it and print the POS tags using the universal tagset."
      ],
      "metadata": {
        "id": "1PEr5Trfl0Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Artificial intelligence transforms modern education.\"\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "DH1ps1khmEgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3: Count Only NOUNs and VERBs**\n",
        "\n",
        "Modify your code to count how many tokens are tagged as NOUN or VERB."
      ],
      "metadata": {
        "id": "Ej5wabnUmKBo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fpVR0swVmXOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§® **Lab Assignment**\n",
        "Complete the following tasks and ensure your answers run without error.\n"
      ],
      "metadata": {
        "id": "cFGaPinHQQ53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **Step 1: Manual Tagging**\n",
        "\n",
        "Go to an English newspaper website (e.g. The Star, New Straits Times).\n",
        "\n",
        "Copy one complete article into a `.txt` file.\n",
        "\n",
        "Select the first sentence and manually tag each word using the UD POS tags (ADJ, NOUN, VERB, etc.). Reference: https://universaldependencies.org/u/pos/\n",
        "\n",
        "Which words were difficult to tag? Why?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "z4XVzjcUiWmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_correction(input_word, dictionary):\n",
        "  #Write your code here"
      ],
      "metadata": {
        "id": "ZcVriQMoMY1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "###**Step 2: Automatic Tagging with NLTK**\n",
        "\n",
        "Using what you learned in above sections, write Python code to:\n",
        "\n",
        "1. Read the article text file from Step 1.\n",
        "2. Tokenize the text.\n",
        "3. Apply POS tagging using NLTK with the Universal tagset.\n",
        "4. Save all tagged tokens into an output text file (`tagged_output.txt`).\n",
        "5. Calculate and print the frequency of each POS tag. Use `Counter()` to count POS tags.\n",
        "\n",
        "Make sure you download required NLTK resources (`punkt`, `averaged_perceptron_tagger`)."
      ],
      "metadata": {
        "id": "IrdkjD0yjCNb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yDO-MiYkpho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ðŸ“¤ **Submission Reminder:**  \n",
        "\n",
        "Submit the following via PutraBlast:\n",
        "\n",
        "- The link to your Colab notebook (as *Viewer link*). Ensure the file name is `LabNo_StudentID_Name.ipynb`.\n",
        "- The article text file (`.txt`)\n",
        "- The tagged output file (`tagged_output.txt`)\n",
        "- Screenshot of your manual tagging  \n",
        "---\n"
      ],
      "metadata": {
        "id": "_xIrY2pJe_kT"
      }
    }
  ]
}